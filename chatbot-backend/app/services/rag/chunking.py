"""
Semantic Chunking Engine for RAG Pipeline

Implements type-specific chunking strategies for PDF, HTML, and text documents
with structure awareness and metadata enrichment.
"""

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from typing import List, Dict, Any, Optional
import re
import logging

logger = logging.getLogger(__name__)


class ChunkingEngine:
    """
    Semantic chunking engine with document-type-specific strategies.

    Supports:
    - PDF: 1200 chars, 200 overlap, page markers, table preservation
    - HTML: 800 chars, 150 overlap, DOM hierarchy, clean content
    - Text: 512 tokens, 200 tokens, sentence boundaries
    """

    def __init__(self):
        """Initialize chunking strategies for each document type."""
        self._pdf_splitter = self._create_pdf_splitter()
        self._html_splitter = self._create_html_splitter()
        self._text_splitter = self._create_text_splitter()

    def _create_pdf_splitter(self) -> RecursiveCharacterTextSplitter:
        """
        Create PDF-specific text splitter.

        Strategy:
        - 1200 characters target size
        - 200 character overlap between chunks
        - Separators: paragraph -> sentence -> word -> character
        - Page markers preserved
        """
        return RecursiveCharacterTextSplitter(
            chunk_size=1200,
            chunk_overlap=200,
            separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""],
            length_function=len,
            is_separator_regex=False,
        )

    def _create_html_splitter(self) -> RecursiveCharacterTextSplitter:
        """
        Create HTML-specific text splitter.

        Strategy:
        - 800 characters target size
        - 150 character overlap
        - DOM-aware splitting (respect heading boundaries)
        """
        return RecursiveCharacterTextSplitter(
            chunk_size=800,
            chunk_overlap=150,
            separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""],
            length_function=len,
            is_separator_regex=False,
        )

    def _create_text_splitter(self) -> RecursiveCharacterTextSplitter:
        """
        Create text-specific text splitter.

        Strategy:
        - 512 tokens target size (approximated as 512*4 = 2048 chars)
        - 200 token overlap
        - Sentence boundary preservation
        """
        # Using character-based approximation for token-based splitting
        # 1 token â‰ˆ 4 characters on average
        return RecursiveCharacterTextSplitter(
            chunk_size=2048,  # Approximate 512 tokens
            chunk_overlap=800,  # Approximate 200 tokens
            separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""],
            length_function=len,
            is_separator_regex=False,
        )

    async def chunk_pdf(self, document: Document) -> List[Document]:
        """
        Chunk PDF document with PDF-specific strategy.

        Args:
            document: LangChain Document with PDF content

        Returns:
            List of chunked Documents with metadata enrichment
        """
        try:
            # Extract page information from metadata
            page_number = document.metadata.get("page_number", 0)
            source_path = document.metadata.get("source_path", "")

            # Split document into chunks
            chunks = self._pdf_splitter.split_documents([document])

            # Enrich each chunk with metadata
            enriched_chunks = []
            for idx, chunk in enumerate(chunks):
                # Detect and preserve table structures
                content = chunk.page_content

                # Check if content contains table-like structure
                is_table = self._detect_table(content)
                if is_table:
                    # Keep table intact, don't split further
                    chunk.metadata["is_table"] = True
                    chunk.metadata["table_warning"] = "Table preserved as atomic unit"

                # Add hierarchy path from headings if available
                hierarchy_path = self._extract_hierarchy_path(document.metadata)
                if hierarchy_path:
                    chunk.metadata["hierarchy_path"] = hierarchy_path

                # Add source page reference
                chunk.metadata["source_page_ref"] = str(page_number + 1)
                chunk.metadata["source_type"] = "pdf"
                chunk.metadata["chunk_index"] = idx
                chunk.metadata["total_chunks"] = len(chunks)

                # Add word and character counts
                chunk.metadata["word_count"] = len(content.split())
                chunk.metadata["char_count"] = len(content)

                enriched_chunks.append(chunk)

            logger.info(f"PDF chunking produced {len(enriched_chunks)} chunks")
            return enriched_chunks

        except Exception as e:
            logger.error(f"PDF chunking failed: {str(e)}")
            raise

    async def chunk_html(self, document: Document) -> List[Document]:
        """
        Chunk HTML document with HTML-specific strategy.

        Args:
            document: LangChain Document with HTML content (already cleaned)

        Returns:
            List of chunked Documents with metadata enrichment
        """
        try:
            # Extract HTML-specific metadata
            title = document.metadata.get("title", "")
            meta_description = document.metadata.get("description", "")
            source_url = document.metadata.get("source_url", "")

            # Clean content - remove any remaining HTML tags
            content = self._clean_html_content(document.page_content)

            # Split into chunks
            chunks = self._html_splitter.split_documents([document])

            # Enrich each chunk with HTML-specific metadata
            enriched_chunks = []
            for idx, chunk in enumerate(chunks):
                # Extract DOM hierarchy from metadata
                dom_path = document.metadata.get("dom_path", [])
                if dom_path:
                    chunk.metadata["hierarchy_path"] = dom_path

                # Add source attribution
                chunk.metadata["source_type"] = "html"
                chunk.metadata["source_url"] = source_url
                chunk.metadata["chunk_index"] = idx
                chunk.metadata["total_chunks"] = len(chunks)
                chunk.metadata["title"] = title
                chunk.metadata["description"] = meta_description

                # Clean content if needed
                chunk.page_content = self._clean_html_content(chunk.page_content)

                # Add counts
                chunk.metadata["word_count"] = len(chunk.page_content.split())
                chunk.metadata["char_count"] = len(chunk.page_content)

                enriched_chunks.append(chunk)

            logger.info(f"HTML chunking produced {len(enriched_chunks)} chunks")
            return enriched_chunks

        except Exception as e:
            logger.error(f"HTML chunking failed: {str(e)}")
            raise

    async def chunk_text(self, document: Document) -> List[Document]:
        """
        Chunk plain text document with text-specific strategy.

        Args:
            document: LangChain Document with text content

        Returns:
            List of chunked Documents with metadata enrichment
        """
        try:
            # Split text preserving sentence boundaries
            chunks = self._text_splitter.split_documents([document])

            # Enrich each chunk with text-specific metadata
            enriched_chunks = []
            for idx, chunk in enumerate(chunks):
                # Ensure no mid-sentence splits
                content = chunk.page_content
                if self._has_mid_sentence_split(content):
                    # Fix by joining with next chunk if possible
                    if idx < len(chunks) - 1:
                        chunk.page_content = (
                            content + " " + chunks[idx + 1].page_content
                        )
                        chunk.page_content = self._ensure_sentence_boundary(
                            chunk.page_content
                        )

                # Add text-specific metadata
                chunk.metadata["source_type"] = "text"
                chunk.metadata["chunk_index"] = idx
                chunk.metadata["total_chunks"] = len(chunks)

                # Add counts
                chunk.metadata["word_count"] = len(content.split())
                chunk.metadata["char_count"] = len(content)

                enriched_chunks.append(chunk)

            logger.info(f"Text chunking produced {len(enriched_chunks)} chunks")
            return enriched_chunks

        except Exception as e:
            logger.error(f"Text chunking failed: {str(e)}")
            raise

    async def chunk_document(self, document: Document, doc_type: str) -> List[Document]:
        """
        Route document to appropriate chunking strategy.

        Args:
            document: LangChain Document to chunk
            doc_type: Document type ('pdf', 'html', 'text')

        Returns:
            List of chunked Documents

        Raises:
            ValueError: If document type is not supported
        """
        if doc_type == "pdf":
            return await self.chunk_pdf(document)
        elif doc_type == "html":
            return await self.chunk_html(document)
        elif doc_type == "text":
            return await self.chunk_text(document)
        else:
            raise ValueError(f"Unsupported document type for chunking: {doc_type}")

    def _detect_table(self, content: str) -> bool:
        """
        Detect if content appears to be a table structure.

        Args:
            content: Text content to analyze

        Returns:
            True if content appears to be a table
        """
        # Check for table-like patterns
        table_patterns = [
            r"\t",  # Tab-separated values
            r"\|\s*\w+",  # Markdown table
            r"\s{2,}\w+",  # Multiple spaces between values
        ]

        for pattern in table_patterns:
            if re.search(pattern, content):
                # Additional check: should be relatively short and structured
                lines = content.split("\n")
                if len(lines) > 1 and len(content) < 2000:
                    return True

        return False

    def _extract_hierarchy_path(self, metadata: Dict[str, Any]) -> List[str]:
        """
        Extract heading hierarchy from document metadata.

        Args:
            metadata: Document metadata containing heading information

        Returns:
            List of heading levels representing hierarchy path
        """
        hierarchy = []

        # Extract from common metadata fields
        if "headings" in metadata:
            for heading in metadata["headings"]:
                if isinstance(heading, str):
                    hierarchy.append(heading)
                elif isinstance(heading, dict) and "text" in heading:
                    hierarchy.append(heading["text"])

        # Extract from specific fields
        if "section" in metadata:
            hierarchy.insert(0, metadata["section"])
        if "chapter" in metadata:
            hierarchy.insert(0, metadata["chapter"])

        return hierarchy

    def _clean_html_content(self, content: str) -> str:
        """
        Clean HTML content by removing any remaining tags.

        Args:
            content: Potentially HTML-contaminated content

        Returns:
            Cleaned text content
        """
        # Remove any remaining HTML tags
        cleaned = re.sub(r"<[^>]+>", "", content)

        # Normalize whitespace
        cleaned = re.sub(r"\s+", " ", cleaned).strip()

        return cleaned

    def _has_mid_sentence_split(self, content: str) -> bool:
        """
        Check if content appears to have been split mid-sentence.

        Args:
            content: Text content to check

        Returns:
            True if split appears mid-sentence
        """
        content = content.strip()

        # Check if content starts with lowercase (after cleanup)
        if content and content[0].islower():
            # This might be a mid-sentence split
            # Check if it follows typical sentence patterns
            words = content.split()
            if len(words) > 0:
                # Common mid-sentence indicators
                return True

        return False

    def _ensure_sentence_boundary(self, content: str) -> str:
        """
        Ensure content ends at a sentence boundary.

        Args:
            content: Text content to fix

        Returns:
            Content adjusted to end at sentence boundary
        """
        # Find the last complete sentence
        sentence_endings = [". ", "! ", "? ", ".\n", "!\n", "?\n"]

        for ending in sentence_endings:
            idx = content.rfind(ending)
            if idx != -1 and idx < len(content) - len(ending):
                # Found a sentence ending, return up to that point
                return content[: idx + 1]

        # If no sentence ending found, try to add one
        if content and not content[-1] in ".!?":
            return content + "."

        return content
